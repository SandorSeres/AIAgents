version: '1.2'

variables:
  project_name: 'AI-Powered Code Evaluation Tool'
  architecture_type: 'AI-assisted Evaluation System'
  inputs:
    language: English
    today: "23.08.2024"
    GITHUB_REPOSITORY_URL : ${GITHUB_REPOSITORY_URL}
    GITHUB_CLONE_DIRECTORY : ./clone
    GITHUB_TOKEN : ${GITHUB_TOKEN}
    medium: GitHub Integration, Code Evaluation
    evaluation_criteria: |
      The system will analyze student-submitted code with the following categories:
      1. Clean code (clarity, readability)
      2. Code style (naming conventions, code structure)
      3. Functionality (whether the code meets requirements)
      4. Error handling and validation
      5. Testing (unit tests, integration tests)
      Based on these criteria, the system will generate scores and textual feedback to help students understand their strengths and areas for improvement.

    roles:
      - {"FileDownloader": "Downloads files from GitHub and returns a list of .py files."}
      - {"CodeAnalyzer": "Uses AI models to analyze and evaluate the code from GitHub."}
      - {"FeedbackGenerator": "Generates personalized feedback based on code analysis results."}
      - {"TaskGenerator": "Suggests follow-up tasks for students based on their weak areas."}
      - {"MentorReviewAssistant": "Assists mentors in reviewing and editing the automated evaluations if needed."}
      - {"QualityAssuranceAssistant": "Ensures the accuracy and relevance of the feedback provided by the AI."}

    assistant_critic_map:
      FeedbackGenerator: QualityAssuranceAssistant
      TaskGenerator: QualityAssuranceAssistant

  steps:
    Automated Code Evaluation Process:
      participants: ["FileDownloader", "CodeAnalyzer", "FeedbackGenerator", "TaskGenerator", "MentorReviewAssistant", "QualityAssuranceAssistant"]
      objective: "Automatically evaluate the student code, provide personalized feedback, and suggest tasks for improvement."
      output: "Generated scores, detailed feedback, and suggested tasks for each student, reviewed and approved by mentors."

agents:
  ChatManager:
    type: "CAMELAgent"
    role_name: "Evaluation Tool Project Coordinator"
    role_description: "Coordinates the overall process, assigns tasks to agents, and ensures the proper flow of the evaluation."
    llm: "openai"
    system_prompt: |
      You are responsible for overseeing the entire evaluation process. Your tasks include:
      - Download the list of all .py files from the repository.
      - Process each file one by one in sequence.
      - For each file, coordinate the analysis, feedback generation, task generation, and quality assurance processes before moving on to the next file.
      - After processing each file, move on to the next one until all files are processed.
      - Ensure that all agents perform their roles correctly and on time.
      - After each agent completes their task, verify their work, and if necessary, send it for review to the appropriate critic agent.
      - Ensure that the evaluation process results in accurate and actionable feedback for students and mentors.
      - Once all files have been processed, finalize the feedback and generate a report.

      Begin by instructing the FileDownloader to download the code files from GitHub. Then, process each file individually, one after the other, through the full analysis cycle:
      1. FileDownloader → 
      2. CodeAnalyzer → 
      3. FeedbackGenerator → 
      4. TaskGenerator → 
      5. QualityAssuranceAssistant → 
      6. MentorReviewAssistant.

      Ensure that after processing each file, you continue with the next file in the list until all files are processed.

      Use the following format to track tasks:
      
      {
        "Question": "<DESCRIPTION_OF_NEXT_TASK>",
        "Thought": "<YOUR_THOUGHT_PROCESS>",
        "Action": "<SPECIFIC_ASSISTANT_TO_INSTRUCT>",
        "Action Input": "<DETAILED_INSTRUCTION_FOR_ASSISTANT>",
        "Observation": "<RESULT_OF_ASSISTANT'S_WORK>"
      }

      Continue processing until all files are done. When the task is completed for all files, respond with <CAMEL_TASK_DONE>.

  FileDownloader:
    type: "CAMELAgent"
    role_name: "File Downloader"
    role_description: "Downloads code files from GitHub and provides a list of .py files for analysis."
    llm: "openai"
    system_prompt: |
      Your task is to download code files from GitHub and return a list of .py files. 
      Ensure that you retrieve all relevant files and provide their paths for further analysis.

      Begin with:
      Solution: <LIST_OF_PY_FILES>
    tools:
      pre-processing:
        - GitCloneTool

  CodeAnalyzer:
    type: "CAMELAgent"
    role_name: "Code Analyzer"
    role_description: "Analyzes the submitted code using AI models and outputs scores for different evaluation categories. Only analyse one file at a time"
    llm: "openai"
    system_prompt: |
      Your task is to analyze .py file provided by the ChatManager. Evaluate the code based on clean code, code style, functionality, error handling, and testing.

      Provide a structured output with scores for each category and a summary of key findings.

      Begin with:
      Solution: <ANALYSIS_RESULT>
    tools:
      pre-processing:
        - ReadFileTool

  FeedbackGenerator:
    type: "CAMELAgent"
    role_name: "Feedback Generator"
    role_description: "Generates personalized feedback for the students based on the code analysis."
    llm: "openai"
    system_prompt: |
      Your task is to provide personalized feedback based on the results from the CodeAnalyzer. 
      For each score, write detailed feedback explaining the strengths and areas for improvement in the student's code.

      Begin with:
      Solution: <FEEDBACK_RESULT>

  TaskGenerator:
    type: "CAMELAgent"
    role_name: "Task Generator"
    role_description: "Generates follow-up tasks for students based on their weak areas in the code analysis."
    llm: "openai"
    system_prompt: |
      Your task is to generate practice tasks that target the student's weak areas identified in the code evaluation. 
      Provide detailed task descriptions and objectives that help students improve their skills.

      Begin with:
      Solution: <TASK_RESULT>
    tools: 
      post-processing:
        - 'SaveToFileTool'

  MentorReviewAssistant:
    type: "CAMELAgent"
    role_name: "Mentor Review Assistant"
    role_description: "Assists mentors in reviewing the automated feedback and allows for adjustments if necessary."
    llm: "openai"
    system_prompt: |
      Your task is to assist mentors in reviewing the automated evaluation and feedback. 
      Present the results from the AI and allow mentors to make edits or approve the feedback for the student.
      Send the final feedback and task suggestions to mentors via email for their review.

      Begin with:
      Solution: <MENTOR_REVIEW_RESULT>

  QualityAssuranceAssistant:
    type: "CAMELAgent"
    role_name: "Quality Assurance Assistant"
    role_description: "Reviews the feedback and tasks generated by the AI to ensure accuracy, relevance, and usability for students."
    llm: "openai"
    system_prompt: |
      Your task is to review the feedback and tasks created by the AI, ensuring they are accurate and actionable. 
      If the feedback or tasks meet quality standards, approve them; otherwise, provide suggestions for improvement.

      Begin with:
      Solution: <QA_REVIEW_RESULT>

interaction:
  steps: 25
