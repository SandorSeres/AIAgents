{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data export\n",
    "docker-compose up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version: '3'\n",
    "services:\n",
    "  db:\n",
    "    image: postgres:15\n",
    "    environment:\n",
    "      POSTGRES_USER: postgres\n",
    "      POSTGRES_PASSWORD: postgres\n",
    "      POSTGRES_DB: cur_learning_module\n",
    "    volumes:\n",
    "      # A teljes sql_files könyvtár csatolása, hogy minden .sql fájl betöltődjön\n",
    "      - ./sql_files:/docker-entrypoint-initdb.d\n",
    "    networks:\n",
    "      - mynetwork\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "\n",
    "  pgadmin:\n",
    "    image: dpage/pgadmin4\n",
    "    environment:\n",
    "      PGADMIN_DEFAULT_EMAIL: sas@code.hu\n",
    "      PGADMIN_DEFAULT_PASSWORD: adminpassword\n",
    "    volumes:\n",
    "      # A teljes sql_files könyvtár csatolása, hogy minden .sql fájl betöltődjön\n",
    "      - ./export_files:/export\n",
    "    ports:\n",
    "      - \"8080:80\"\n",
    "    depends_on:\n",
    "      - db\n",
    "    networks:\n",
    "      - mynetwork\n",
    "\n",
    "networks:\n",
    "  mynetwork:\n",
    "    driver: bridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB Export scripts\n",
    "#### Learning Moduls\n",
    "select id, title, learning_units from public.cur_learning_module ;\n",
    "#### Projects\n",
    "select id, title, story,  learn,  tasks, hints, background from public.cur_base_project ;\n",
    "#### Tutorial\n",
    "select id, title, description, question_id from public.cur_background_material ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_moduls = pd.read_csv(\"../csv/modules.csv\")\n",
    "projects = pd.read_csv(\"../csv/projects.csv\")\n",
    "tutorials = pd.read_csv(\"../csv/tutorials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"week\": null, \"title\": \"10th week\", \"topics\": null, \"description\": null, \"learningUnitAssignments\": [{\"id\": 5694, \"type\": \"TUTORIAL\", \"optional\": false}, {\"id\": 15628, \"type\": \"PROJECT\", \"optional\": false, \"assignmentType\": \"SOLO\"}, {\"id\": 15306, \"type\": \"PROJECT\", \"optional\": false, \"assignmentType\": \"SOLO\"}]}, {\"week\": null, \"title\": \"11th week\", \"topics\": null, \"description\": null, \"learningUnitAssignments\": [{\"id\": 5026, \"type\": \"TUTORIAL\", \"optional\": false}, {\"id\": 5721, \"type\": \"TUTORIAL\", \"optional\": false}, {\"id\": 5012, \"type\": \"TUTORIAL\", \"optional\": false}, {\"id\": 15294, \"type\": \"TUTORIAL\", \"optional\": false}, {\"id\": 15460, \"type\": \"PROJECT\", \"optional\": false, \"assignmentType\": \"SOLO\"}, {\"id\": 15282, \"type\": \"PROJECT\", \"optional\": false, \"assignmentType\": \"SOLO\"}]}, {\"week\": null, \"title\": \"12th week\", \"topics\": null, \"description\": null, \"learningUnitAssignments\": [{\"id\": 15254, \"type\": \"PROJECT\", \"optional\": false, \"assignmentType\": \"SOLO\"}]}]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_moduls.learning_units[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'story', 'learn', 'tasks', 'hints', 'background'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LearningUnitAssignments frissítésére\n",
    "def update_assignment(assignment, projects_df, tutorials_df):\n",
    "    if assignment['type'] == 'TUTORIAL':\n",
    "        # Keressük meg a tutorials_df-ben a megfelelő leírást az id alapján\n",
    "        tutorial_row = tutorials_df[tutorials_df['id'] == assignment['id']]\n",
    "        if not tutorial_row.empty:\n",
    "            struct = {\"type\" : \"TUTORIAL\"}\n",
    "            struct['title']= tutorial_row.iloc[0]['title']\n",
    "            struct['description']= tutorial_row.iloc[0]['description']\n",
    "            struct['question_id']= tutorial_row.iloc[0]['question_id']\n",
    "            return struct\n",
    "    elif assignment['type'] == 'PROJECT':\n",
    "        # Keressük meg a projects_df-ben a megfelelő hintet az id alapján\n",
    "        project_row = projects_df[projects_df['id'] == assignment['id']]\n",
    "        if not project_row.empty:\n",
    "            struct = {\"type\" : \"PROJECT\"}\n",
    "            struct['title']= project_row.iloc[0]['title']\n",
    "            struct['story']= project_row.iloc[0]['story']\n",
    "            struct['learn']= project_row.iloc[0]['learn']\n",
    "            struct['tasks']= project_row.iloc[0]['tasks']\n",
    "            struct['hints']= project_row.iloc[0]['hints']\n",
    "            struct['background']= project_row.iloc[0]['background']\n",
    "            return struct\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LearninUnit feldolgozása\n",
    "def update_learning_units(row):\n",
    "    updated_units = []\n",
    "    jrow = json.loads(row)  # JSON betöltése\n",
    "    for r in jrow:\n",
    "        lus = r[\"learningUnitAssignments\"]\n",
    "        updated_assignments = []\n",
    "        for lu in lus[:5]:\n",
    "            # Frissítjük a learningUnitAssignments bejegyzést a megfelelő értékekkel\n",
    "            updated_assignments.append(update_assignment(lu, projects, tutorials))\n",
    "        # Frissítjük az eredeti bejegyzést az új értékekkel\n",
    "        r[\"learningUnitAssignments\"] = updated_assignments\n",
    "    return jrow \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all= []\n",
    "id_list = [40652, 39052, 35202, 35852, 31652, 31653, 23452, 39802, 29404, 38552, 31102, 32858]\n",
    "#selected = learning_moduls[:2]\n",
    "selected = learning_moduls[learning_moduls['id'].isin(id_list)]\n",
    "for i, t, k in zip (selected[\"id\"], selected[\"title\"],selected[\"learning_units\"]):\n",
    "    lus = update_learning_units(k)\n",
    "    all.append([i,t,lus ])\n",
    "all = pd.DataFrame(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning outcome generation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "import openai\n",
    "from openai import OpenAIError\n",
    "import tiktoken\n",
    "\n",
    "load_dotenv()\n",
    "MAX_TOKENS = 100000\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(lu, encoding):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in the message list.\n",
    "    \n",
    "    Parameters:\n",
    "        messages (list): List of learning units.\n",
    "        encoding (tiktoken.Encoding): The encoding used for tokens.\n",
    "    \n",
    "    Returns:\n",
    "        int: The total number of tokens.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Summing the token counts for each unit\n",
    "        count = sum(len(encoding.encode(json.dumps(unit))) for unit in lu)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in counting tokens: {e}\")\n",
    "        count = 0\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_string(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a JSON string from a block of text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text containing the JSON string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted JSON string, or an empty JSON object/list if extraction fails.\n",
    "\n",
    "    Logs:\n",
    "        Warning: If there is an error during JSON extraction.\n",
    "    \"\"\"\n",
    "    def find_matching_end(text, start_pos, open_char, close_char):\n",
    "        stack = 1\n",
    "        for i in range(start_pos + 1, len(text)):\n",
    "            if text[i] == open_char:\n",
    "                stack += 1\n",
    "            elif text[i] == close_char:\n",
    "                stack -= 1\n",
    "                if stack == 0:\n",
    "                    return i + 1\n",
    "        return -1  # No matching closing character found\n",
    "\n",
    "    try:\n",
    "        # Find the first occurrence of '{' or '['\n",
    "        first_brace = text.find('{')\n",
    "        first_bracket = text.find('[')\n",
    "\n",
    "        if first_brace == -1 and first_bracket == -1:\n",
    "            raise ValueError(\"No JSON object or array found in the text.\")\n",
    "\n",
    "        if first_brace == -1:\n",
    "            start = first_bracket\n",
    "            open_char = '['\n",
    "            close_char = ']'\n",
    "        elif first_bracket == -1:\n",
    "            start = first_brace\n",
    "            open_char = '{'\n",
    "            close_char = '}'\n",
    "        else:\n",
    "            if first_brace < first_bracket:\n",
    "                start = first_brace\n",
    "                open_char = '{'\n",
    "                close_char = '}'\n",
    "            else:\n",
    "                start = first_bracket\n",
    "                open_char = '['\n",
    "                close_char = ']'\n",
    "\n",
    "        end = find_matching_end(text, start, open_char, close_char)\n",
    "        if end == -1:\n",
    "            raise ValueError(\"No matching closing character found for JSON.\")\n",
    "\n",
    "        json_str = text[start:end]\n",
    "        return json.loads(json_str)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"JSON extraction error: {e}; text: {text}\")\n",
    "        # Determine what to return based on the first opening character\n",
    "        if 'open_char' in locals():\n",
    "            if open_char == '{':\n",
    "                return {}\n",
    "            elif open_char == '[':\n",
    "                return []\n",
    "        return {} # Default to empty JSON object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_data_with_llm(self, learning_units):\n",
    "    \"\"\"\n",
    "    Summarizes a set of learning units using the LLM.\n",
    "    \n",
    "    Parameters:\n",
    "        learning_units (list): A list of learningUnitAssignments to summarize.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the summarized text.\n",
    "    \"\"\"\n",
    "    print(\"Summarise since it is too long to process!\")\n",
    "    # Join all learning unit contents together\n",
    "    text_data = \" \".join([json.dumps(unit) for unit in learning_units])\n",
    "\n",
    "    # System message to guide the summarization\n",
    "    sys_msg = \"\"\"\n",
    "    You are a tool for summarizing and abstracting text. \n",
    "    Your task is to reduce the provided data to less than 10000 words using markdown format. \n",
    "    The generated summary should maintain key information and follow the original language of the text.\n",
    "    \"\"\"\n",
    "\n",
    "    # If you're using OpenAI, send the summarization request\n",
    "    summary = self.query_openai([{'role': 'system', 'content': sys_msg}, {'role': 'user', 'content': text_data}])\n",
    "    \n",
    "    return {'role': 'system', 'content': summary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10), retry=retry_if_exception_type(OpenAIError))\n",
    "def query_openai(messages, max_response_tokens=32000):\n",
    "    \"\"\"\n",
    "    Queries the OpenAI API with the given messages, using retries for reliability.\n",
    "\n",
    "    Parameters:\n",
    "        messages (list): A list of messages to send to the OpenAI API.\n",
    "        max_response_tokens (int): The maximum number of tokens for the response.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the response message.\n",
    "\n",
    "    Raises:\n",
    "        OpenAIError: If an error occurs during the API call.\n",
    "    \"\"\"\n",
    "    # Calculate total tokens in the input messages\n",
    "    total_tokens = count_tokens(messages, encoding)\n",
    "\n",
    "    if total_tokens > MAX_TOKENS:\n",
    "        print(f\"Token limit exceeded ({total_tokens} tokens). Summarizing Learning unit.\")\n",
    "        \n",
    "        # Summarize the data if token count exceeds the limit\n",
    "        summarized_message = summarize_data_with_llm(messages)\n",
    "\n",
    "        # Recalculate the tokens after summarization\n",
    "        total_tokens = count_tokens([summarized_message], encoding)\n",
    "        print(f\"Token count after summarization: {total_tokens}\")\n",
    "\n",
    "        # Replace the original messages with summarized content\n",
    "        messages = [summarized_message]\n",
    "    \n",
    "    try:\n",
    "        # Query the OpenAI API with the final set of messages\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=os.getenv(\"MODEL\"),\n",
    "            messages=messages,\n",
    "            #max_tokens=max_response_tokens,\n",
    "            temperature=float(os.getenv(\"TEMPERATURE\"))\n",
    "        )\n",
    "        #print(f\"OpenAI query successful.\")\n",
    "        return completion.choices[0].message.content\n",
    "    except OpenAIError as e:\n",
    "        print(f\"OpenAIError: {e}\", exc_info=True)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_learning_outcomes(module_title, learning_units):\n",
    "    \"\"\"\n",
    "    Generates learning outcomes for a specific module using the OpenAI API.\n",
    "    \n",
    "    Parameters:\n",
    "        module_title (str): The title of the learning module.\n",
    "        learning_units (list or str): A list of learning units or a combined string of learning outcomes.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated learning outcomes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Alap prompt létrehozása\n",
    "    base_prompt = \"\"\"\n",
    "    Develop 4-6 measurable learning outcomes for a computer course module on {module_title}, utilizing \n",
    "    - Bloom's Taxonomy \n",
    "    - reasoning \n",
    "    - independent thinking.\n",
    "    \n",
    "    Each outcome should:\n",
    "    \n",
    "    Use one measurable action verb from Bloom's Taxonomy (e.g., remember, understand, apply, analyze, evaluate, create).\n",
    "    Be clear, concise, and student-centered.\n",
    "    Align with the appropriate Bloom's level for the course (e.g., introductory, intermediate, advanced).\n",
    "    Avoid using vague terms like 'understand' or 'learn.'\n",
    "    Collectively, the learning outcomes should cover a range of Bloom's levels suitable for the students' level.\n",
    "    Incorporate Reasoning and Independent Thinking by:\n",
    "    \n",
    "    **Understanding the Topic**\n",
    "    \n",
    "    Break down the topic into core components, identifying key areas for student exploration.\n",
    "    Place concepts within broader academic, social, or historical contexts to deepen understanding.\n",
    "    \n",
    "    **Critical Analysis**\n",
    "    \n",
    "    Encourage evaluation of the reliability, bias, and relevance of information sources.\n",
    "    Promote assessment of logical consistency in arguments or problem-solving approaches.\n",
    "    \n",
    "    **Independent Reasoning**\n",
    "    \n",
    "    Prompt generation of original hypotheses or solutions beyond standard answers.\n",
    "    Facilitate connections between course concepts and broader contexts, suggesting implications or applications.\n",
    "    Challenge students to address contradictions or gaps in knowledge to foster deeper analysis.\n",
    "    \n",
    "    **Synthesizing and Concluding**\n",
    "    \n",
    "    Require synthesis of information into coherent arguments or theories.\n",
    "    Encourage reflection on how learned concepts support or challenge existing knowledge.\n",
    "    Suggest areas for further study based on identified gaps or limitations.\n",
    "\n",
    "    Important: You should never add additional text to your response, just only the measurable learning outcomes!\n",
    "    \"\"\"\n",
    "\n",
    "    # Ha a learning_units string típusú, akkor azt közvetlenül használjuk\n",
    "    if isinstance(learning_units, str):\n",
    "        learning_units_text = learning_units\n",
    "    else:\n",
    "        # Lista esetén az egyes learning unitok címének összefűzése\n",
    "        learning_units_text = \"\\n\".join([f\"- {unit.get('title', 'N/A')}\" for unit in learning_units])\n",
    "    \n",
    "    # Végleges prompt összeállítása\n",
    "    final_prompt = base_prompt.format(module_title=module_title) + \"\\nLearning units included:\\n\" + learning_units_text\n",
    "\n",
    "    # Itt hívjuk meg az OpenAI API-t, hogy learning outcome-okat generáljon\n",
    "    response = query_openai([{'role': 'user', 'content': final_prompt}])\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_modules_for_outcomes(selected_modules):\n",
    "    \"\"\"\n",
    "    Processes the selected modules to generate learning outcomes for each module and each learning unit.\n",
    "\n",
    "    Parameters:\n",
    "        selected_modules (DataFrame): The DataFrame containing selected modules.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame containing the module ID, title, learning unit ID, and generated learning outcomes.\n",
    "    \"\"\"\n",
    "    all_outcomes = []\n",
    "    \n",
    "    # Végigmegyünk az 'all' DataFrame minden során, és létrehozzuk a learning outcome-okat\n",
    "    for module_id, module_title, learning_units_json in zip(selected_modules[0], selected_modules[1], selected_modules[2]):\n",
    "        print(f\"Processing module: {module_id} - {module_title}\")\n",
    "        \n",
    "        # Ellenőrizzük, hogy a learning units JSON-e, és ha szükséges, átalakítjuk\n",
    "        if isinstance(learning_units_json, str):\n",
    "            try:\n",
    "                learning_units = json.loads(learning_units_json)  # JSON stringből Python objektummá alakítás\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Hiba történt a JSON feldolgozása során: {learning_units_json}\")\n",
    "                continue\n",
    "        else:\n",
    "            learning_units = learning_units_json\n",
    "        \n",
    "        assignments = learning_units\n",
    "\n",
    "        if not isinstance(assignments, list):\n",
    "            print(f\"Nem megfelelő a learningUnitAssignments formátuma: {assignments}\")\n",
    "            continue\n",
    "\n",
    "        # 1. Minden learning unitra külön learning outcome generálása\n",
    "        unit_outcomes = []  # Itt gyűjtjük az egyes learning unit outcome-okat\n",
    "        for idx, assignment in enumerate(assignments):\n",
    "            if 'title' not in assignment:\n",
    "                assignment['title'] = \"N/A\"  # Ha hiányzik, adjunk neki egy alapértéket\n",
    "            \n",
    "            # Generálunk outcome-ot minden learning unitra\n",
    "            unit_outcome = generate_learning_outcomes(assignment['title'], [assignment])\n",
    "            print(f\"Learning unit outcome: {unit_outcome}\")\n",
    "            \n",
    "            # Tároljuk az eredményeket a megfelelő learning unit ID-val\n",
    "            all_outcomes.append([module_id, module_title, idx, unit_outcome])\n",
    "            \n",
    "            # Összegyűjtjük a learning unit outcome-okat a modul összegzéshez\n",
    "            unit_outcomes.append(unit_outcome)\n",
    "\n",
    "        # 2. Modul szintű összegző learning outcome készítése a learning unit outcome-ok alapján\n",
    "        combined_unit_outcomes = \" \".join(unit_outcomes)  # Egyesítjük a learning unit outcome-okat\n",
    "        summary_outcome = generate_learning_outcomes(module_title, combined_unit_outcomes)\n",
    "        print(f\"Modul összegző outcome: {summary_outcome}\")\n",
    "        \n",
    "        # Modul szintű összegzés hozzáadása\n",
    "        all_outcomes.append([module_id, module_title, \"Module Summary\", summary_outcome])\n",
    "\n",
    "    # Eredmény DataFrame létrehozása\n",
    "    outcome_df = pd.DataFrame(all_outcomes, columns=[\"Module ID\", \"Module Title\", \"Learning_Unit_ID\", \"Learning Outcomes\"])\n",
    "    \n",
    "    return outcome_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df = process_modules_for_outcomes(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df.to_csv(\"outcomes.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
